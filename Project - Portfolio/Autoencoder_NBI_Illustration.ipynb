{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d079845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pip installation may be needed ONLY IF importing yfinance does not work\n",
    "# In which case, do it before executing the import statement below.\n",
    "\n",
    "# pip install yfinance\n",
    "\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2f48717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f6d4eef",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\farid1\\\\OneDrive - University of Florida\\\\FinTech_ML\\\\MS Business Analytics\\\\Auto_encoding\\\\Project'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# get raw data on IBBQ, an ETF that tracks the NBI\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Data in spreadsheet downloaded from Invesco, the manager of IBBQ\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mfarid1\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mOneDrive - University of Florida\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mFinTech_ML\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mMS Business Analytics\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mAuto_encoding\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mProject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m raw_IBBQ \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mIBBQ_holdings_3.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize of the dataset (row, col): \u001b[39m\u001b[38;5;124m\"\u001b[39m, raw_IBBQ\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\farid1\\\\OneDrive - University of Florida\\\\FinTech_ML\\\\MS Business Analytics\\\\Auto_encoding\\\\Project'"
     ]
    }
   ],
   "source": [
    "# get raw data on IBBQ, an ETF that tracks the NBI\n",
    "# Data in spreadsheet downloaded from Invesco, the manager of IBBQ\n",
    "os.chdir(r\"C:\\Users\\farid1\\OneDrive - University of Florida\\FinTech_ML\\MS Business Analytics\\Auto_encoding\\Project\")\n",
    "raw_IBBQ = pd.read_csv(\".\\IBBQ_holdings_3.csv\")\n",
    "\n",
    "\n",
    "print(\"Size of the dataset (row, col): \", raw_IBBQ.shape)\n",
    "raw_IBBQ.head()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf7175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get daily open/high/low/closing values for NBI over the past 5 years\n",
    "NBI_values = yf.download('^NBI',start='2018-06-13',end='2023-06-13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc9f11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBI_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb51112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get NBI closing values\n",
    "NBI_close = NBI_values[[\"Close\"]]\n",
    "\n",
    "NBI_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4474b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Stocks that form the NBI \n",
    "IBBQ_tickers = raw_IBBQ['Holding Ticker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2490b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "IBBQ_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03881d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that if you reference elements of IBBQ_tickers, you will get a trailing whitespace\n",
    "# that can create a problem when reading in the corresponding data through yfinance as a block\n",
    "# For example,\n",
    "ticker=IBBQ_tickers[0]\n",
    "# leads to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4b5b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd156f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Therefore, the solution below removes such whitespace and prevents the issue \n",
    "\n",
    "tickers=[]\n",
    "for ticker in IBBQ_tickers:\n",
    "    tickers.append(ticker.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da07d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, if we check the ticker label for AMGN, we have\n",
    "tickers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now get all the 5-year price data for the stocks in the NBI\n",
    "df_data_download = yf.download(tickers,start='2018-06-13',end='2023-06-13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78c5130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get a basic, global idea of the data\n",
    "df_data_download.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10da614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now a look more closely\n",
    "df_data_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6796aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove the columns we do not need\n",
    "df_data_download = df_data_download.drop(columns=['Open', 'High', 'Low', 'Close','Volume'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985eb16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_download # Notice below that we have gone from 1620 columns to 270 (exactly the number of stocks in NBI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f3fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the 'Adj Close' in the column header, making it multiindex, and to keep matters simple and\n",
    "# helpful, we should eliminate this additional index. To do so, first relabel it so that we can use\n",
    "# it to redefine a new dataframe\n",
    "df_data_download.rename(columns={'Adj Close':'adj_close'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d59508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the dataframe\n",
    "df_data_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9711a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now define a new dataframe with just a single index column name\n",
    "\n",
    "df_returns = df_data_download.adj_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d94fdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b0e73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  We are now in position to compute the log returns \n",
    "#  To simplify the typing below, lets rename the dataframe for our returns\n",
    "df2 = df_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f94cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the log returns and replace the columns entries, which previously were closing prices,\n",
    "# with the log-returns (and relabel the columns as well)\n",
    "for i in range(len(df2.columns)):\n",
    "    df2.loc[:,df2.columns[i]]=np.log(df2.loc[:,df2.columns[i]]/df2.loc[:,df2.columns[i]].shift(1))\n",
    "    df2.rename(columns={df2.columns[i]:df2.columns[i]+' log retn'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d537abff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at the returns just computed\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651f70b5",
   "metadata": {},
   "source": [
    "# Autoencoder Model\n",
    "## Part 1: \n",
    "##   Find the \"most communal\" and \"least communal stocks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf510a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We could have imported the libraries in this cell in the first (or second) one above\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow import keras\n",
    "\n",
    "from numpy.random import seed\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5517a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into training set and test set\n",
    "train,test=train_test_split(df2,test_size=0.2,random_state=100)\n",
    "\n",
    "# Divide training set into training and validation set\n",
    "train,validation=train_test_split(train,test_size=0.25,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf5b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the size of the data for the NN modeling \n",
    "print(\"train:\",train.shape)\n",
    "print(\"validation:\",validation.shape)\n",
    "print(\"test:\",test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb53d026",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_stocks = len(df2.columns)\n",
    "num_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff20c4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Autoencoder Model\n",
    "\n",
    "# Here we create a model for each stock and, for each, save the resulting MSE\n",
    "# We then rank teh stocks on the basis of the MSE\n",
    "\n",
    "model=list(range(num_stocks))\n",
    "mse_test=[]\n",
    "\n",
    "for i in range (num_stocks):\n",
    "    x_train=train.iloc[:,i].dropna()\n",
    "    y_train=x_train\n",
    "    x_val=validation.iloc[:,i].dropna()\n",
    "    y_val=x_val\n",
    "    x_test=test.iloc[:,i].dropna()\n",
    "    y_test=x_test\n",
    "    \n",
    "    model[i] = keras.models.Sequential([Dense(20,activation = \"relu\",input_shape = (1,)),\n",
    "                                 Dense(5,activation = \"relu\"),\n",
    "                                 Dense(20,activation = \"relu\"),\n",
    "                                 Dense(1,activation = \"sigmoid\")])\n",
    "    \n",
    "    model[i].compile(loss = \"mse\",optimizer = \"Adam\")\n",
    "\n",
    "    history=model[i].fit(x_train,y_train,\n",
    "                      epochs=100, \n",
    "                      batch_size = 128, \n",
    "                      validation_data= (x_val,y_val),\n",
    "                      verbose = 0)\n",
    "    \n",
    " # Calcuate  and save MSE for testing set   \n",
    "    mse_test.append(model[i].evaluate(x_test,y_test,verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b1bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we list the stock in increasing order of MSE\n",
    "print(\"stock #  |   mse   |      stock name\")\n",
    "ranking = np.array(mse_test).argsort()\n",
    "for stock_index in ranking:\n",
    "    print(stock_index, mse_test[stock_index], df2.iloc[:,stock_index].name) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed2f88e",
   "metadata": {},
   "source": [
    "# Part 2:\n",
    "## Build a tracking portfolio based on \n",
    "### the 10 \"most communal\" and \n",
    "### the 15 \"least communal stocks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57193ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select 10 most communal and 15 least communal stocks\n",
    "non_communal=15\n",
    "\n",
    "# In this case we have a total of s stocks, where\n",
    "s = 10 + non_communal  \n",
    "\n",
    "stock_index = np.concatenate((ranking[0:10], ranking[-non_communal:]))\n",
    "stock_index   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c988600",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Portfolio with 25 stocks\n",
    "port25 = df2.iloc[:, stock_index]\n",
    "port25=port25.fillna(0)\n",
    "port25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c35c340",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate index log return\n",
    "idx_ret=np.log(NBI_close.pct_change()+1)\n",
    "idx_ret=idx_ret.fillna(0)\n",
    "idx_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62608273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into training set and test set\n",
    "X_train1,X_test1,Y_train1,Y_test1=train_test_split(port25,idx_ret,test_size=0.2,random_state=100)\n",
    "\n",
    "# Divide training set into training and validation set\n",
    "X_train1,X_val1,Y_train1,Y_val1=train_test_split(X_train1,Y_train1,test_size=0.25,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b20e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NN Index-Tracking Model 1\n",
    "model1 = keras.models.Sequential([Dense(10,activation = \"relu\",input_shape = (25,)),\n",
    "                                 Dense(10,activation = \"relu\"),\n",
    "                                 Dense(25,activation = \"softmax\")])\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6562d51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function to capture the (Euclidean) distance between the index performance and that \n",
    "# of the tracking portfolio above \n",
    "\n",
    "def custom_loss_function (x_values, y_values):\n",
    "      squared_difference = tf.square(x_values-y_values) \n",
    "      return tf.reduce_mean(squared_difference, axis=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd6cd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile NN Model 1\n",
    "model1.compile(loss = custom_loss_function, optimizer = \"Adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62cff89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Model 1 and collect performance in history 1\n",
    "history1 = model1.fit(X_train1, Y_train1,\n",
    "                     epochs=500, \n",
    "                     batch_size = 128, \n",
    "                     validation_data=(X_val1,Y_val1),\n",
    "                     verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae664f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get weights for stocks in the portfolio\n",
    "weights1 = model1.predict(X_test1)\n",
    "weights1 = weights1[0]\n",
    "weights1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae926c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate portfolio returns\n",
    "port_ret1 = np.dot(port25,weights1)\n",
    "port_ret1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70557a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare portfolio returns with 25 stocks with index returns\n",
    "cl1 = custom_loss_function(np.array(port_ret1), np.array(idx_ret))\n",
    "cl1.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f69cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean error (loss) between index returns and index-tracking portfolio with a total of S = 25 stocks selected above\n",
    "mean_loss1=np.mean(cl1)\n",
    "mean_loss1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a601bf2b",
   "metadata": {},
   "source": [
    "# Part 2:\n",
    "## Build a tracking portfolio based on \n",
    "### the 10 \"most communal\" and \n",
    "### the 35 \"least communal stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c920a68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select 10 most communal and 15 least communal stocks\n",
    "non_communal=35\n",
    "\n",
    "# In this case we have a total of s stocks, where\n",
    "s = 10 + non_communal  \n",
    "\n",
    "stock_index = np.concatenate((ranking[0:10], ranking[-non_communal:]))\n",
    "stock_index   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2b16d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Portfolio with 45 stocks\n",
    "port45 = df2.iloc[:, stock_index]\n",
    "port45=port45.fillna(0)\n",
    "port45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7537872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide data into training set and test set\n",
    "X_train2,X_test2,Y_train2,Y_test2=train_test_split(port45,idx_ret,test_size=0.2,random_state=100)\n",
    "\n",
    "# Divide training set into training and validation set\n",
    "X_train2,X_val2,Y_train2,Y_val2=train_test_split(X_train2,Y_train2,test_size=0.25,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d610d02d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Index Tracking Model 2\n",
    "model2 = keras.models.Sequential([Dense(10,activation = \"relu\",input_shape = (45,)),\n",
    "                                 Dense(10,activation = \"relu\"),\n",
    "                                 Dense(45,activation = \"softmax\")])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d77e38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile Model 2\n",
    "model2.compile(loss = custom_loss_function, optimizer = \"Adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884cf557",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history2 = model2.fit(X_train2, Y_train2,\n",
    "                     epochs=500, \n",
    "                     batch_size = 128, \n",
    "                     validation_data=(X_val2,Y_val2),\n",
    "                     verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579cf35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get portfolio weights for stocks\n",
    "weights2 = model2.predict(X_test2)\n",
    "weights2 = weights2[0]\n",
    "weights2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5508dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate portfolio returns\n",
    "port_ret2 = np.dot(port45,weights2)\n",
    "port_ret2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4807df77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare portfolio returns with 45 stocks with index returns\n",
    "cl2 = custom_loss_function(np.array(port_ret2), np.array(idx_ret))\n",
    "cl2.numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9e1eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean error (loss) between index returns and index-tracking portfolio with a total of S = 45 stocks selected above\n",
    "mean_loss2=np.mean(cl2)\n",
    "mean_loss2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52b0b5c",
   "metadata": {},
   "source": [
    "# Plot the returns of the index versus those of the two portfolios identified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0321257b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the returns of the index versus portfolio 1\n",
    "# Number of time observations is:\n",
    "numobs = len(NBI_values)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [12,8] \n",
    "plt.plot(range(0,numobs),idx_ret,color='green', label='idx_ret')\n",
    "plt.plot(range(0,numobs),port_ret1,color='red', label='port_ret1')\n",
    "\n",
    "plt.xticks(range(1,numobs,2), fontsize = 18)\n",
    "plt.yticks(fontsize = 18)\n",
    "plt.ylabel(\"Returns\",fontsize = 18)\n",
    "plt.xlabel(\"time\", fontsize = 18)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53d03eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the returns of the index versus portfolio 2\n",
    "plt.rcParams['figure.figsize'] = [12,8] \n",
    "plt.plot(range(0,numobs),idx_ret,color='green', label='idx_ret')\n",
    "plt.plot(range(0,numobs),port_ret2,color='blue', label='port_ret2')\n",
    "plt.xticks(range(1,numobs,2), fontsize = 18)\n",
    "plt.yticks(fontsize = 18)\n",
    "plt.ylabel(\"Returns\",fontsize = 18)\n",
    "plt.xlabel(\"time\", fontsize = 18)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
